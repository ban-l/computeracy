# 大量数据问题

处理“大量数据”的 SQL 操作是生产环境中最容易引发事故的场景。

如果一次性操作的数据量达到几十万甚至上百万行，可能会导致**数据库锁死、事务日志爆满（Undo Log 膨胀）、甚至拖垮整个应用服务（OOM）**。

作为 Java 开发工程师，需要从 **DML（写/改）** 和 **DQL（读）** 两个维度采取完全不同的策略。

以下是针对大批量数据操作的专家级优化方案：

---

## 场景一：大批量写入 (INSERT)

如果你需要一次性插入 10 万条数据，千万不要在 Java 代码里写 `for` 循环一条条 `insert`。

### 1. 使用“多值插入” (Multi-Value Insert)

减少网络交互（Round-Trip）是核心。

* **低效**：执行 1000 次 SQL，每次插 1 行。
* **高效**：执行 1 次 SQL，每次插 1000 行。

```sql
-- 推荐写法：拼接 VALUES
INSERT INTO user_logs (uid, action, time) VALUES 
(101, 'login', '2023-01-01'),
(102, 'logout', '2023-01-01'),
... (这里可以放 500-1000 条) ... ;
```

### 2. JDBC 批处理优化 (Java 特别技巧)

即使你在 MyBatis 或 JPA 中开启了 `Batch` 模式，MySQL 驱动默认可能还是会一条条发。

你必须在 JDBC 连接 URL 中添加参数：`&rewriteBatchedStatements=true`

> **原理**：这个参数会欺骗数据库驱动，让它在底层自动把你的多条 `INSERT` 语句重写成上面的 `VALUES (), (), ()` 形式，性能提升通常在 10 倍以上。

### 3. 显式开启事务

不要让每条插入都自动提交（Auto Commit）。建议每 1000~2000 条数据提交一次事务。

* **原因**：每次提交事务都需要强制刷盘（Redo Log 落盘），开销巨大。合并提交可以大幅减少 I/O。

---

## 场景二：大批量更新/删除 (UPDATE / DELETE)

这是最危险的操作。例如：`DELETE FROM logs WHERE create_time < '2023-01-01';` 

如果涉及几百万行，会导致**长时间的表锁或行锁**，甚至导致主从同步延迟。

### 1. “切片”操作 (Chunking) —— 核心方案

永远不要在一个事务中处理所有数据。利用**主键 ID** 或 **LIMIT** 进行分批处理。

* **错误**：一次性删除 100 万行。
* **正确**：写一个循环（在 Java 或存储过程中），每次删 5000 行，删完歇一会儿（sleep 50ms），防止占满 CPU。

```sql
-- 伪代码逻辑
WHILE (rows > 0) DO
    DELETE FROM logs 
    WHERE create_time < '2023-01-01' 
    LIMIT 5000; -- 每次只删 5000
    -- 提交事务，释放锁
    COMMIT;
    -- 暂停一下，给其他业务留活路
    SLEEP(0.05);
END WHILE;

```

### 2. 如果是清空全表

直接使用 `TRUNCATE TABLE`。

* `TRUNCATE` 是 DDL 操作，它不记录单行删除的日志，而是直接重建表文件，速度极快（毫秒级），但**不可回滚**。

---

## 场景三：大批量读取 (SELECT)

如果你需要把 100 万条数据导出到 Excel 或进行计算，直接 `SELECT *` 会导致 Java 应用堆内存溢出（OOM）。

### 1. 流式查询 (Stream Query)

这是解决 OOM 的终极办法。默认情况下，JDBC 会把查询结果全部读入内存。你需要配置让它“读一行，处理一行”。

* **MySQL (Java 写法)**：
```java
stmt = conn.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
stmt.setFetchSize(Integer.MIN_VALUE); // 关键黑魔法
```


设置 `FetchSize` 为 `Integer.MIN_VALUE` 会告诉 MySQL 驱动不要一次性拉取所有数据，而是建立流式游标。

### 2. 游标分页 (Cursor-based Pagination)

我在之前的回答中提到了深分页问题。对于大批量导出，不要用 `LIMIT offset, size`，要用 `WHERE id > last_id`。

* **第一次**：`SELECT * FROM users WHERE id > 0 LIMIT 1000;` (记录最后一条 ID 为 1000)
* **第二次**：`SELECT * FROM users WHERE id > 1000 LIMIT 1000;`
* **优势**：无论翻到多少页，原本的 B+ 树索引定位速度都是恒定的。

---

## 总结

| 操作类型 | 核心瓶颈 | 优化策略 (一句话) | Java 侧配合 |
| --- | --- | --- | --- |
| **大批量 INSERT** | 网络交互 (RTT) & 日志刷盘 | **拼接 VALUES** + **合并事务** (每1000条一提交) | `rewriteBatchedStatements=true` |
| **大批量 UPDATE/DELETE** | 锁竞争 & Undo Log 膨胀 | **分批切片 (Chunking)** + **LIMIT** | 编写 Loop 循环，并在批次间短暂 Sleep |
| **大批量 SELECT** | 内存溢出 (OOM) | **流式查询** 或 **主键游标** | `setFetchSize(Integer.MIN_VALUE)` |
